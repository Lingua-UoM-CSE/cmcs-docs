"use strict";(self.webpackChunkcmcs_docs=self.webpackChunkcmcs_docs||[]).push([[531],{4137:(e,t,n)=>{n.d(t,{Zo:()=>g,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=r.createContext({}),l=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},g=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,p=e.parentName,g=s(e,["components","mdxType","originalType","parentName"]),u=l(n),c=a,d=u["".concat(p,".").concat(c)]||u[c]||m[c]||i;return n?r.createElement(d,o(o({ref:t},g),{},{components:n})):r.createElement(d,o({ref:t},g))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=c;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var l=2;l<i;l++)o[l]=n[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},8977:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=n(7462),a=(n(7294),n(4137));const i={slug:"Understanding_Language_Model_Prompting",title:"Understanding Language Model Prompting",authors:["indunil-19"],tags:["Language models","Prompting","NLP"]},o=void 0,s={permalink:"/cmcs-docs/blog/Understanding_Language_Model_Prompting",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-04-19.md",source:"@site/blog/2023-04-19.md",title:"Understanding Language Model Prompting",description:"Language models (LMs) are powerful tools for natural language processing tasks. However, traditional supervised learning approaches require a large amount of labeled data, which is often not available for many tasks. In this context, prompting has emerged as a popular approach for training LMs in low-data scenarios.",date:"2023-04-19T00:00:00.000Z",formattedDate:"April 19, 2023",tags:[{label:"Language models",permalink:"/cmcs-docs/blog/tags/language-models"},{label:"Prompting",permalink:"/cmcs-docs/blog/tags/prompting"},{label:"NLP",permalink:"/cmcs-docs/blog/tags/nlp"}],readingTime:1.965,hasTruncateMarker:!0,authors:[{name:"Indunil Udayangana",title:"Lingua maintainer",url:"https://github.com/indunil-19",imageURL:"https://avatars.githubusercontent.com/u/63807666?v=4",key:"indunil-19"}],frontMatter:{slug:"Understanding_Language_Model_Prompting",title:"Understanding Language Model Prompting",authors:["indunil-19"],tags:["Language models","Prompting","NLP"]},prevItem:{title:"Prompting Basics",permalink:"/cmcs-docs/blog/prompting_basics"},nextItem:{title:"Prompt Engineering",permalink:"/cmcs-docs/blog/Prompt_Engineering"}},p={authorsImageUrls:[void 0]},l=[{value:"Types of Prompting",id:"types-of-prompting",level:2},{value:"\ud83d\udfe3 Tuning-free Prompting",id:"-tuning-free-prompting",level:3},{value:"\ud83d\udfe2 Fixed-LM Prompt Tuning",id:"-fixed-lm-prompt-tuning",level:3},{value:"\ud83d\udfe1 Fixed-prompt LM Tuning",id:"-fixed-prompt-lm-tuning",level:3},{value:"\ud83d\udd35 Prompt+LM Tuning",id:"-promptlm-tuning",level:3},{value:"Applications of Prompting",id:"applications-of-prompting",level:2}],g={toc:l},u="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},g,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{src:"https://img.shields.io/badge/Language%20Model-Prompting-blueviolet"})),(0,a.kt)("p",null,"Language models (LMs) are powerful tools for natural language processing tasks. However, traditional supervised learning approaches require a large amount of labeled data, which is often not available for many tasks. In this context, prompting has emerged as a popular approach for training LMs in low-data scenarios."),(0,a.kt)("h2",{id:"types-of-prompting"},"Types of Prompting"),(0,a.kt)("h3",{id:"-tuning-free-prompting"},"\ud83d\udfe3 Tuning-free Prompting"),(0,a.kt)("p",null,"Tuning-free prompting is an efficient approach that doesn't require any parameter update process. The LM parameters remain fixed, eliminating the problem of catastrophic forgetting. It's also applicable in zero-shot settings, making it an attractive option for some applications. However, it requires heavy engineering to achieve high accuracy, particularly in the in-context learning setting. Additionally, it's challenging to use large training datasets because providing many answered prompts can be slow at test time."),(0,a.kt)("h3",{id:"-fixed-lm-prompt-tuning"},"\ud83d\udfe2 Fixed-LM Prompt Tuning"),(0,a.kt)("p",null,"Fixed-LM prompt tuning is similar to tuning-free prompting but can be used in few-shot scenarios to achieve superior accuracy. It's not applicable in zero-shot scenarios, and the representation power is limited in large-data settings. Prompt engineering through the choice of hyperparameters or seed prompts is necessary. Prompts are usually not human-interpretable or manipulable (soft prompts)."),(0,a.kt)("h3",{id:"-fixed-prompt-lm-tuning"},"\ud83d\udfe1 Fixed-prompt LM Tuning"),(0,a.kt)("p",null,"In fixed-prompt LM tuning, prompt or answer engineering more completely specifies the task, allowing for more efficient learning, particularly in few-shot scenarios. However, prompt or answer engineering is still required, although perhaps not as much as without prompting. LMs fine-tuned on one downstream task may not be effective on another one."),(0,a.kt)("h3",{id:"-promptlm-tuning"},"\ud83d\udd35 Prompt+LM Tuning"),(0,a.kt)("p",null,"Prompt+LM tuning is the most expressive method and likely suitable for high-data settings. It's very similar to the standard pre-train and fine-tune paradigm, but the addition of the prompt can provide additional bootstrapping at the start of model training. However, it requires training and storing all parameters of the models and may overfit to small datasets."),(0,a.kt)("h2",{id:"applications-of-prompting"},"Applications of Prompting"),(0,a.kt)("p",null,"In prompting, the language models can be used for different tasks such as question answering, summarization, knowledge probing, sentiment analysis, and classification using a single LM model. However, if we fine-tune LM, we can only use it for the target task in which LM was tuned."),(0,a.kt)("p",null,"Prompting has opened up many possibilities for low-data NLP tasks. With a better understanding of the different types of prompting, researchers and practitioners can choose the most suitable approach for their specific use cases."))}m.isMDXComponent=!0}}]);
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Code-mixed Text Classification Blog</title>
        <link>https://lingua-uom-cse.github.io/cmcs-docs/blog</link>
        <description>Code-mixed Text Classification Blog</description>
        <lastBuildDate>Thu, 20 Apr 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Prompting Basics]]></title>
            <link>https://lingua-uom-cse.github.io/cmcs-docs/blog/prompting_basics</link>
            <guid>https://lingua-uom-cse.github.io/cmcs-docs/blog/prompting_basics</guid>
            <pubDate>Thu, 20 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[To understand the different types of prompting methods, it is important to first understand the basics of prompting notation. The following table summarizes the different components of prompting notation:]]></description>
            <content:encoded><![CDATA[<p>To understand the different types of prompting methods, it is important to first understand the basics of prompting notation. The following table summarizes the different components of prompting notation:</p><table><thead><tr><th>Name</th><th>Notation</th><th>Example</th><th>Description</th></tr></thead><tbody><tr><td>Input</td><td>x</td><td>"I love this movie."</td><td>One or multiple texts</td></tr><tr><td>Output</td><td>y</td><td>++ (very positive)</td><td>Output label or text</td></tr><tr><td>Prompting Function</td><td>fprompt(x)</td><td>[X]<!-- --> Overall, it was a <!-- -->[Z]<!-- --> movie.</td><td>A function that converts the input into a specific form by inserting the input x and adding a slot <!-- -->[Z]<!-- --> where answer z may be filled later.</td></tr><tr><td>Prompt</td><td>x'</td><td>"I love this movie. Overall, it was a <!-- -->[Z]<!-- --> movie."</td><td>A text where <!-- -->[X]<!-- --> is instantiated by input x but answer slot <!-- -->[Z]<!-- --> is not.</td></tr><tr><td>Filled Prompt</td><td>ffill(x', z)</td><td>"I love this movie. Overall, it was a bad movie."</td><td>A prompt where slot <!-- -->[Z]<!-- --> is filled with any answer.</td></tr><tr><td>Answered Prompt</td><td>ffill(x', z<!-- -->*<!-- -->)</td><td>"I love this movie. Overall, it was a good movie."</td><td>A prompt where slot <!-- -->[Z]<!-- --> is filled with a true answer.</td></tr><tr><td>Answer</td><td>z</td><td>"good", "fantastic", "boring"</td><td>A token, phrase, or sentence that fills <!-- -->[Z]<!-- -->.</td></tr></tbody></table><p>In this table, <code>x</code> represents the input text, which can be one or multiple texts. <code>y</code> represents the output label or text that the prompting method is trying to generate. The prompting function <code>fprompt(x)</code> is a function that takes in input <code>x</code> and returns a prompt <code>x'</code> with a slot <code>[Z]</code> for an answer. The filled prompt <code>ffill(x', z)</code> is a prompt where the slot <code>[Z]</code> is filled with any answer <code>z</code>. The answered prompt <code>ffill(x', z*)</code> is a prompt where the slot <code>[Z]</code> is filled with a true answer <code>z*</code>. The answer <code>z</code> is a token, phrase, or sentence that fills the slot <code>[Z]</code>.</p><p>By using this notation, we can create different types of prompts for different tasks, such as question answering, summarization, sentiment analysis, and classification, using a single language model.</p>]]></content:encoded>
            <category>Prompting</category>
        </item>
        <item>
            <title><![CDATA[Understanding Language Model Prompting]]></title>
            <link>https://lingua-uom-cse.github.io/cmcs-docs/blog/Understanding_Language_Model_Prompting</link>
            <guid>https://lingua-uom-cse.github.io/cmcs-docs/blog/Understanding_Language_Model_Prompting</guid>
            <pubDate>Wed, 19 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Language models (LMs) are powerful tools for natural language processing tasks. However, traditional supervised learning approaches require a large amount of labeled data, which is often not available for many tasks. In this context, prompting has emerged as a popular approach for training LMs in low-data scenarios.]]></description>
            <content:encoded><![CDATA[<p align="center"><img loading="lazy" src="https://img.shields.io/badge/Language%20Model-Prompting-blueviolet" class="img_ev3q"></p><p>Language models (LMs) are powerful tools for natural language processing tasks. However, traditional supervised learning approaches require a large amount of labeled data, which is often not available for many tasks. In this context, prompting has emerged as a popular approach for training LMs in low-data scenarios.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="types-of-prompting">Types of Prompting<a href="#types-of-prompting" class="hash-link" aria-label="Direct link to Types of Prompting" title="Direct link to Types of Prompting">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-tuning-free-prompting">ðŸŸ£ Tuning-free Prompting<a href="#-tuning-free-prompting" class="hash-link" aria-label="Direct link to ðŸŸ£ Tuning-free Prompting" title="Direct link to ðŸŸ£ Tuning-free Prompting">â€‹</a></h3><p>Tuning-free prompting is an efficient approach that doesn't require any parameter update process. The LM parameters remain fixed, eliminating the problem of catastrophic forgetting. It's also applicable in zero-shot settings, making it an attractive option for some applications. However, it requires heavy engineering to achieve high accuracy, particularly in the in-context learning setting. Additionally, it's challenging to use large training datasets because providing many answered prompts can be slow at test time.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-fixed-lm-prompt-tuning">ðŸŸ¢ Fixed-LM Prompt Tuning<a href="#-fixed-lm-prompt-tuning" class="hash-link" aria-label="Direct link to ðŸŸ¢ Fixed-LM Prompt Tuning" title="Direct link to ðŸŸ¢ Fixed-LM Prompt Tuning">â€‹</a></h3><p>Fixed-LM prompt tuning is similar to tuning-free prompting but can be used in few-shot scenarios to achieve superior accuracy. It's not applicable in zero-shot scenarios, and the representation power is limited in large-data settings. Prompt engineering through the choice of hyperparameters or seed prompts is necessary. Prompts are usually not human-interpretable or manipulable (soft prompts).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-fixed-prompt-lm-tuning">ðŸŸ¡ Fixed-prompt LM Tuning<a href="#-fixed-prompt-lm-tuning" class="hash-link" aria-label="Direct link to ðŸŸ¡ Fixed-prompt LM Tuning" title="Direct link to ðŸŸ¡ Fixed-prompt LM Tuning">â€‹</a></h3><p>In fixed-prompt LM tuning, prompt or answer engineering more completely specifies the task, allowing for more efficient learning, particularly in few-shot scenarios. However, prompt or answer engineering is still required, although perhaps not as much as without prompting. LMs fine-tuned on one downstream task may not be effective on another one.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-promptlm-tuning">ðŸ”µ Prompt+LM Tuning<a href="#-promptlm-tuning" class="hash-link" aria-label="Direct link to ðŸ”µ Prompt+LM Tuning" title="Direct link to ðŸ”µ Prompt+LM Tuning">â€‹</a></h3><p>Prompt+LM tuning is the most expressive method and likely suitable for high-data settings. It's very similar to the standard pre-train and fine-tune paradigm, but the addition of the prompt can provide additional bootstrapping at the start of model training. However, it requires training and storing all parameters of the models and may overfit to small datasets.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications-of-prompting">Applications of Prompting<a href="#applications-of-prompting" class="hash-link" aria-label="Direct link to Applications of Prompting" title="Direct link to Applications of Prompting">â€‹</a></h2><p>In prompting, the language models can be used for different tasks such as question answering, summarization, knowledge probing, sentiment analysis, and classification using a single LM model. However, if we fine-tune LM, we can only use it for the target task in which LM was tuned.</p><p>Prompting has opened up many possibilities for low-data NLP tasks. With a better understanding of the different types of prompting, researchers and practitioners can choose the most suitable approach for their specific use cases.</p>]]></content:encoded>
            <category>Language models</category>
            <category>Prompting</category>
            <category>NLP</category>
        </item>
        <item>
            <title><![CDATA[Prompt Engineering]]></title>
            <link>https://lingua-uom-cse.github.io/cmcs-docs/blog/Prompt_Engineering</link>
            <guid>https://lingua-uom-cse.github.io/cmcs-docs/blog/Prompt_Engineering</guid>
            <pubDate>Tue, 18 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[The recent advances in language models have led to remarkable improvements in various NLP tasks, such as text generation, summarization, question answering, and sentiment analysis. However, even state-of-the-art language models like GPT-3 require massive amounts of training data and compute resources to achieve their impressive performance. In this context, prompt engineering is a promising approach that can significantly improve the efficiency and effectiveness of language models for specific tasks.]]></description>
            <content:encoded><![CDATA[<p>The recent advances in language models have led to remarkable improvements in various NLP tasks, such as text generation, summarization, question answering, and sentiment analysis. However, even state-of-the-art language models like GPT-3 require massive amounts of training data and compute resources to achieve their impressive performance. In this context, prompt engineering is a promising approach that can significantly improve the efficiency and effectiveness of language models for specific tasks.</p><p>Prompt engineering involves designing effective prompts that can guide the language model to produce the desired output for a given task. There are two main types of prompts: prefix prompts and cloze prompts. Prefix prompts involve adding task-specific vectors to the input text and updating only the prefix during training. This approach has been used in various text generation and text classification tasks. Cloze prompts involve converting subject-relation-object triples or question-answer pairs into a cloze statement that can be used to query the language model.</p><p>Designing the best prompt for each task can be done manually or automatically. Manual template designing has been used in many studies, but it has some limitations, as inappropriate prompts can lead to lower performance or incomplete knowledge contained in the language model. Automated template designing can be done using discrete or continuous prompts. Discrete prompts involve defining a set of trigger tokens that can be learned using gradient-based search strategies. Continuous prompts involve optimizing continuous vectors that are prepended to the input text. This approach, called pre-fix tuning, has been used in various studies to optimize language models for specific tasks while keeping the pre-trained parameters frozen.</p><p>To sum up, prompt engineering is a promising approach to improve the performance of language models for specific tasks. Effective prompts can significantly reduce the training data and compute resources required to achieve state-of-the-art performance, while also enhancing the interpretability and transparency of language models. Further research is needed to explore the optimal prompt engineering methods for various NLP tasks and to develop automated methods for prompt template designing.</p>]]></content:encoded>
            <category>PromptEngineering</category>
        </item>
    </channel>
</rss>
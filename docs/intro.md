---
sidebar_position: 1
---

# ‚≠ê Introduction

In modern communication systems, it is common to see people switching between languages or mixing two languages in a single conversation. This phenomenon, known as code-mixing and code-switching, is especially prevalent in social media platforms such as Facebook, Twitter, and Instagram, as well as messaging applications like WhatsApp and Viber. With users from all over the world bringing their own cultures and backgrounds to the mix, it is not surprising that this trend has become so widespread.

Code-mixing is the practice of borrowing words from one language and adapting them to another without affecting the topic, while code-switching refers to the juxtaposition of two grammatical systems or subsystems within the same conversation. These linguistic components can include words, phrases, and morphemes, among others.

The use of code-mixing and code-switching is crucial for effective advertising, providing customer assistance, and gathering user feedback on products. In some cases, limiting communication to a single language may be challenging if one party is not fluent in that language. However, the handling of code-mixed and code-switched (CMCS) data presents significant challenges for Natural Language Processing (NLP) due to several reasons.

Firstly, NLP tools developed for a single language may underperform in the context of CMCS data. Secondly, there is a lack of annotated datasets, a significant number of unobserved constructions created by combining the syntax and lexicon of two or more languages, and a large number of possible CMCS combinations. This problem is further exacerbated in the context of low-resource languages (LRLs), where datasets are even more scarce, and NLP tools are sub-optimal.

To overcome these challenges, pre-trained multilingual language models (PMLMs) such as mBERT and XLM-R have been developed. These models have attained state-of-the-art performance in most text classification tasks, including code-mixed data classification. In previous work, PMLMs were mostly used with some basic fine-tuning and hyperparameter optimization.

Despite the success of PMLMs, there is still a need for more research to improve their performance on CMCS data. Additionally, more efforts are required to build annotated datasets for LRLs, which would enable the development of better NLP tools for these languages.
